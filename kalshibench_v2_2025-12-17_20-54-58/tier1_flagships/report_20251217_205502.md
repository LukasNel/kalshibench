# KalshiBench Evaluation Report

**Generated:** 2025-12-17 21:11:31  
**Benchmark Version:** 1.0

---

## Executive Summary

KalshiBench evaluates language model forecasting ability and calibration using temporally-filtered prediction market questions from Kalshi, a CFTC-regulated prediction market.

| Metric | Value |
|--------|-------|
| Total Questions | 300 |
| Knowledge Cutoff | 2025-10-01 |
| Models Evaluated | 5 |
| Date Range | 2025-10-01T03:59:00Z to 2025-11-16T23:00:00Z |
| Yes Rate | 40.0% |
| Random Seed | 42 |


## Dataset Analysis

### Overview
- **Total Questions:** 300
- **Knowledge Cutoff:** 2025-10-01
- **Source:** Kalshi (CFTC-regulated prediction market)

### Temporal Distribution

| Metric | Value |
|--------|-------|
| Date Range | 2025-10-01T03:59:00Z to 2025-11-16T23:00:00Z |
| Temporal Span | 46 days |
| Mean Time Horizon | 34.0 days |
| Median Time Horizon | 37.0 days |
| Std Dev Time Horizon | 11.3 days |
| Min Time Horizon | 2 days |
| Max Time Horizon | 46 days |

#### Questions by Month

| Month | Count | Yes Rate |
|-------|-------|----------|
| 2025-10 | 85 | 56.5% |
| 2025-11 | 215 | 33.5% |


### Ground Truth Distribution

| Outcome | Count | Percentage |
|---------|-------|------------|
| Yes | 120 | 40.0% |
| No | 180 | 60.0% |

### Category Distribution

- **Number of Categories:** 13
- **Category Entropy:** 3.01 bits (max = 3.70 bits)
- **Largest Category:** Sports (83 questions)
- **Smallest Category:** Science and Technology (1 questions)

| Category | Count | % | Yes Rate |
|----------|-------|---|----------|
| Sports | 83 | 27.7% | 34.9% |
| Politics | 55 | 18.3% | 52.7% |
| Entertainment | 47 | 15.7% | 36.2% |
| Companies | 30 | 10.0% | 60.0% |
| Elections | 24 | 8.0% | 20.8% |
| Mentions | 19 | 6.3% | 36.8% |
| Crypto | 11 | 3.7% | 27.3% |
| Climate and Weather | 9 | 3.0% | 33.3% |
| Financials | 8 | 2.7% | 12.5% |
| World | 6 | 2.0% | 50.0% |
| Economics | 4 | 1.3% | 50.0% |
| Social | 3 | 1.0% | 66.7% |
| Science and Technology | 1 | 0.3% | 100.0% |


### Question Complexity

| Metric | Mean | Median | Std | Min | Max |
|--------|------|--------|-----|-----|-----|
| Question Length (chars) | 61 | 58 | 26 | 19 | 288 |
| Description Length (chars) | 442 | 406 | 321 | 77 | 1488 |

- **Total Vocabulary Size:** 2,224 unique words
- **Average Words per Question:** 85.8



---

## Model Comparison Summary

### Classification Performance

| Model | Accuracy | Macro F1 | Precision (Yes) | Recall (Yes) | F1 (Yes) | Parse Rate |
|-------|----------|----------|-----------------|--------------|----------|------------|
| Claude-Opus-4.5 | 69.33% | 0.676 | 0.633 | 0.570 | 0.600 | 100.00% |
| DeepSeek-V3.2 | 64.33% | 0.614 | 0.573 | 0.455 | 0.507 | 100.00% |
| GPT-5.2-XHigh | 65.33% | 0.599 | 0.623 | 0.355 | 0.453 | 100.00% |
| Qwen3-235B-Thinking | 65.67% | 0.607 | 0.625 | 0.372 | 0.466 | 100.00% |
| Kimi-K2 | 67.12% | 0.633 | 0.630 | 0.436 | 0.515 | 97.33% |


### Calibration Metrics

| Model | Brier | BSS | ECE | MCE | ACE | Log Loss |
|-------|-------|-----|-----|-----|-----|----------|
| Claude-Opus-4.5 | 0.2269 | 0.0570 | 0.1201 | 0.2460 | 0.1170 | 0.6614 |
| DeepSeek-V3.2 | 0.3386 | -0.4070 | 0.2838 | 0.6297 | 0.2728 | 1.0891 |
| GPT-5.2-XHigh | 0.4329 | -0.7988 | 0.3947 | 0.6222 | 0.3947 | 1.4133 |
| Qwen3-235B-Thinking | 0.3459 | -0.4372 | 0.2966 | 0.4790 | 0.3000 | 1.2726 |
| Kimi-K2 | 0.3473 | -0.4461 | 0.2981 | 0.5705 | 0.2889 | 1.0835 |


**Metric Definitions:**
- **Brier:** Brier Score - mean squared error of probability predictions (lower = better)
- **BSS:** Brier Skill Score - improvement over always predicting base rate (higher = better)
- **ECE:** Expected Calibration Error - avg |confidence - accuracy| weighted by bin size (lower = better)
- **MCE:** Maximum Calibration Error - worst-calibrated bin (lower = better)
- **ACE:** Adaptive Calibration Error - ECE with equal-mass bins (lower = better)

### Confidence Analysis

| Model | Avg Conf | Conf When Correct | Conf When Wrong | Overconf@70% | Overconf@80% | Overconf@90% |
|-------|----------|-------------------|-----------------|--------------|--------------|---------------|
| Claude-Opus-4.5 | 73.78% | 75.00% | 71.02% | 27.06% | 23.08% | 20.75% |
| DeepSeek-V3.2 | 73.65% | 76.11% | 69.21% | 24.67% | 23.60% | 14.71% |
| GPT-5.2-XHigh | 80.12% | 81.82% | 76.90% | 30.29% | 28.28% | 27.66% |
| Qwen3-235B-Thinking | 81.66% | 82.31% | 80.42% | 32.26% | 32.56% | 32.35% |
| Kimi-K2 | 79.35% | 80.84% | 76.31% | 25.91% | 29.86% | 31.11% |


**Overconfidence Rate @X%:** Fraction of wrong predictions among those with confidence > X%

### Token Usage & Cost

| Model | Input Tokens | Output Tokens | Total Tokens | Cost (USD) |
|-------|--------------|---------------|--------------|------------|
| Claude-Opus-4.5 | 86,070 | 137,901 | 223,971 | $11.6336 |
| DeepSeek-V3.2 | 77,317 | 226,736 | 304,053 | $0.3649 |
| GPT-5.2-XHigh | 79,132 | 1,994,703 | 2,073,835 | $30.3162 |
| Qwen3-235B-Thinking | 83,267 | 510,814 | 594,081 | $1.1882 |
| Kimi-K2 | 79,112 | 544,743 | 623,855 | $0.9358 |
| **TOTAL** | - | - | **3,819,795** | **$44.4387** |



## Leaderboards

### By Accuracy (Higher is Better)

| Rank | Model | Accuracy |
|------|-------|----------|
| 1 | Claude-Opus-4.5 | 69.33% |
| 2 | Kimi-K2 | 67.12% |
| 3 | Qwen3-235B-Thinking | 65.67% |
| 4 | GPT-5.2-XHigh | 65.33% |
| 5 | DeepSeek-V3.2 | 64.33% |

### By Brier Score (Lower is Better)

| Rank | Model | Brier Score |
|------|-------|-------------|
| 1 | Claude-Opus-4.5 | 0.2269 |
| 2 | DeepSeek-V3.2 | 0.3386 |
| 3 | Qwen3-235B-Thinking | 0.3459 |
| 4 | Kimi-K2 | 0.3473 |
| 5 | GPT-5.2-XHigh | 0.4329 |

### By ECE (Lower is Better)

| Rank | Model | ECE |
|------|-------|-----|
| 1 | Claude-Opus-4.5 | 0.1201 |
| 2 | DeepSeek-V3.2 | 0.2838 |
| 3 | Qwen3-235B-Thinking | 0.2966 |
| 4 | Kimi-K2 | 0.2981 |
| 5 | GPT-5.2-XHigh | 0.3947 |



---

# Detailed Model Results


---

## Claude-Opus-4.5

**Model Configuration:**
- **Provider:** ModelProvider.ANTHROPIC
- **Model ID:** anthropic/claude-opus-4-5-20251101
- **Knowledge Cutoff:** 2025-04-01
- **Temperature:** 0.7

### All Metrics

#### Classification Metrics

| Metric | Value |
|--------|-------|
| Accuracy | 0.6933 (69.33%) |
| Macro F1 | 0.6757 |
| Precision (Yes) | 0.6330 |
| Recall (Yes) | 0.5702 |
| F1 (Yes) | 0.6000 |
| Precision (No) | 0.7277 |
| Recall (No) | 0.7765 |
| F1 (No) | 0.7514 |

#### Calibration Metrics

| Metric | Value | Interpretation |
|--------|-------|----------------|
| Brier Score | 0.2269 | Lower is better (0 = perfect) |
| Brier Skill Score | 0.0570 | Higher is better (improvement over base rate) |
| ECE | 0.1201 | Lower is better (expected calibration error) |
| MCE | 0.2460 | Lower is better (max calibration error) |
| ACE | 0.1170 | Lower is better (adaptive calibration error) |
| Log Loss | 0.6614 | Lower is better |

#### Confidence Analysis

| Metric | Value |
|--------|-------|
| Average Confidence | 0.7378 |
| Avg Confidence When Correct | 0.75 |
| Avg Confidence When Wrong | 0.7102173913043477 |
| Overconfidence Rate @70% | 0.27058823529411763 |
| Overconfidence Rate @80% | 0.23076923076923078 |
| Overconfidence Rate @90% | 0.20754716981132076 |

#### Performance Metrics

| Metric | Value |
|--------|-------|
| Parse Rate | 1.0000 (100.00%) |
| Avg Latency | 11784.3 ms |

#### Token Usage & Cost

| Metric | Value |
|--------|-------|
| Input Tokens | 86,070 |
| Output Tokens | 137,901 |
| Total Tokens | 223,971 |
| Total Cost | $11.6336 |

### Confusion Matrix


```
                 Predicted
              Yes        No
           ┌─────────┬─────────┐
Actual Yes │     69   │     52   │
           ├─────────┼─────────┤
Actual No  │     40   │    139   │
           └─────────┴─────────┘
```


- **True Positives (TP):** 69 - Correctly predicted "yes"
- **True Negatives (TN):** 139 - Correctly predicted "no"
- **False Positives (FP):** 40 - Incorrectly predicted "yes" (actual was "no")
- **False Negatives (FN):** 52 - Incorrectly predicted "no" (actual was "yes")

### Reliability Diagram

Shows calibration: ideally, avg_accuracy should equal avg_confidence in each bin.

| Bin | Avg Confidence | Avg Accuracy | Count | Gap |
|-----|----------------|--------------|-------|-----|
| 0.0-0.1 | 0.054 | 0.194 | 36 | -0.141 |
| 0.1-0.2 | 0.151 | 0.188 | 32 | -0.037 |
| 0.2-0.3 | 0.248 | 0.333 | 42 | -0.085 |
| 0.3-0.4 | 0.359 | 0.355 | 31 | +0.004 |
| 0.4-0.5 | 0.439 | 0.333 | 36 | +0.106 |
| 0.5-0.6 | 0.566 | 0.353 | 34 | +0.213 |
| 0.6-0.7 | 0.641 | 0.724 | 29 | -0.083 |
| 0.7-0.8 | 0.751 | 0.542 | 24 | +0.210 |
| 0.8-0.9 | 0.854 | 0.688 | 16 | +0.167 |
| 0.9-1.0 | 0.946 | 0.700 | 20 | +0.246 |


**Interpretation:**
- **Gap > 0:** Model is overconfident (confidence > accuracy)
- **Gap < 0:** Model is underconfident (confidence < accuracy)
- **Gap ≈ 0:** Well-calibrated

### Category Breakdown

| Category | Accuracy | Brier Score | Count |
|----------|----------|-------------|-------|
| Sports | 75.90% | 0.1930 | 83 |
| Politics | 67.27% | 0.2693 | 55 |
| Entertainment | 78.72% | 0.1866 | 47 |
| Companies | 60.00% | 0.2570 | 30 |
| Elections | 75.00% | 0.1721 | 24 |
| Mentions | 52.63% | 0.3573 | 19 |
| Crypto | 36.36% | 0.2404 | 11 |
| Climate and Weather | 77.78% | 0.2291 | 9 |
| Financials | 75.00% | 0.2032 | 8 |
| World | 50.00% | 0.2621 | 6 |
| Economics | 50.00% | 0.3259 | 4 |
| Social | 100.00% | 0.0107 | 3 |
| Science and Technology | 0.00% | 0.6084 | 1 |



---

## DeepSeek-V3.2

**Model Configuration:**
- **Provider:** ModelProvider.FIREWORKS
- **Model ID:** fireworks_ai/accounts/fireworks/models/deepseek-v3p2
- **Knowledge Cutoff:** 2025-10-01
- **Temperature:** 0.7

### All Metrics

#### Classification Metrics

| Metric | Value |
|--------|-------|
| Accuracy | 0.6433 (64.33%) |
| Macro F1 | 0.6138 |
| Precision (Yes) | 0.5729 |
| Recall (Yes) | 0.4545 |
| F1 (Yes) | 0.5069 |
| Precision (No) | 0.6765 |
| Recall (No) | 0.7709 |
| F1 (No) | 0.7206 |

#### Calibration Metrics

| Metric | Value | Interpretation |
|--------|-------|----------------|
| Brier Score | 0.3386 | Lower is better (0 = perfect) |
| Brier Skill Score | -0.4070 | Higher is better (improvement over base rate) |
| ECE | 0.2838 | Lower is better (expected calibration error) |
| MCE | 0.6297 | Lower is better (max calibration error) |
| ACE | 0.2728 | Lower is better (adaptive calibration error) |
| Log Loss | 1.0891 | Lower is better |

#### Confidence Analysis

| Metric | Value |
|--------|-------|
| Average Confidence | 0.7365 |
| Avg Confidence When Correct | 0.7610880829015544 |
| Avg Confidence When Wrong | 0.6920560747663551 |
| Overconfidence Rate @70% | 0.24666666666666667 |
| Overconfidence Rate @80% | 0.23595505617977527 |
| Overconfidence Rate @90% | 0.14705882352941177 |

#### Performance Metrics

| Metric | Value |
|--------|-------|
| Parse Rate | 1.0000 (100.00%) |
| Avg Latency | 7946.7 ms |

#### Token Usage & Cost

| Metric | Value |
|--------|-------|
| Input Tokens | 77,317 |
| Output Tokens | 226,736 |
| Total Tokens | 304,053 |
| Total Cost | $0.3649 |

### Confusion Matrix


```
                 Predicted
              Yes        No
           ┌─────────┬─────────┐
Actual Yes │     55   │     66   │
           ├─────────┼─────────┤
Actual No  │     41   │    138   │
           └─────────┴─────────┘
```


- **True Positives (TP):** 55 - Correctly predicted "yes"
- **True Negatives (TN):** 138 - Correctly predicted "no"
- **False Positives (FP):** 41 - Incorrectly predicted "yes" (actual was "no")
- **False Negatives (FN):** 66 - Incorrectly predicted "no" (actual was "yes")

### Reliability Diagram

Shows calibration: ideally, avg_accuracy should equal avg_confidence in each bin.

| Bin | Avg Confidence | Avg Accuracy | Count | Gap |
|-----|----------------|--------------|-------|-----|
| 0.0-0.1 | 0.048 | 0.200 | 10 | -0.152 |
| 0.1-0.2 | 0.175 | 0.250 | 8 | -0.075 |
| 0.2-0.3 | 0.250 | 0.000 | 4 | +0.250 |
| 0.3-0.4 | 0.344 | 0.333 | 9 | +0.011 |
| 0.4-0.5 | 0.418 | 0.545 | 11 | -0.127 |
| 0.5-0.6 | 0.575 | 0.365 | 63 | +0.210 |
| 0.6-0.7 | 0.673 | 0.463 | 67 | +0.211 |
| 0.7-0.8 | 0.747 | 0.400 | 30 | +0.347 |
| 0.8-0.9 | 0.831 | 0.517 | 58 | +0.313 |
| 0.9-1.0 | 0.937 | 0.308 | 39 | +0.630 |


**Interpretation:**
- **Gap > 0:** Model is overconfident (confidence > accuracy)
- **Gap < 0:** Model is underconfident (confidence < accuracy)
- **Gap ≈ 0:** Well-calibrated

### Category Breakdown

| Category | Accuracy | Brier Score | Count |
|----------|----------|-------------|-------|
| Sports | 65.06% | 0.3503 | 83 |
| Politics | 54.55% | 0.2680 | 55 |
| Entertainment | 76.60% | 0.4761 | 47 |
| Companies | 60.00% | 0.2668 | 30 |
| Elections | 83.33% | 0.3634 | 24 |
| Mentions | 52.63% | 0.3009 | 19 |
| Crypto | 54.55% | 0.3257 | 11 |
| Climate and Weather | 66.67% | 0.3208 | 9 |
| Financials | 62.50% | 0.3013 | 8 |
| World | 66.67% | 0.2833 | 6 |
| Economics | 50.00% | 0.2737 | 4 |
| Social | 66.67% | 0.3225 | 3 |
| Science and Technology | 0.00% | 0.3025 | 1 |



---

## GPT-5.2-XHigh

**Model Configuration:**
- **Provider:** ModelProvider.OPENAI
- **Model ID:** openai/gpt-5.2
- **Knowledge Cutoff:** 2025-10-01
- **Temperature:** 1.0
- **Reasoning Effort:** xhigh

### All Metrics

#### Classification Metrics

| Metric | Value |
|--------|-------|
| Accuracy | 0.6533 (65.33%) |
| Macro F1 | 0.5995 |
| Precision (Yes) | 0.6232 |
| Recall (Yes) | 0.3554 |
| F1 (Yes) | 0.4526 |
| Precision (No) | 0.6623 |
| Recall (No) | 0.8547 |
| F1 (No) | 0.7463 |

#### Calibration Metrics

| Metric | Value | Interpretation |
|--------|-------|----------------|
| Brier Score | 0.4329 | Lower is better (0 = perfect) |
| Brier Skill Score | -0.7988 | Higher is better (improvement over base rate) |
| ECE | 0.3947 | Lower is better (expected calibration error) |
| MCE | 0.6222 | Lower is better (max calibration error) |
| ACE | 0.3947 | Lower is better (adaptive calibration error) |
| Log Loss | 1.4133 | Lower is better |

#### Confidence Analysis

| Metric | Value |
|--------|-------|
| Average Confidence | 0.8012 |
| Avg Confidence When Correct | 0.8182448979591836 |
| Avg Confidence When Wrong | 0.7690384615384617 |
| Overconfidence Rate @70% | 0.30288461538461536 |
| Overconfidence Rate @80% | 0.2827586206896552 |
| Overconfidence Rate @90% | 0.2765957446808511 |

#### Performance Metrics

| Metric | Value |
|--------|-------|
| Parse Rate | 1.0000 (100.00%) |
| Avg Latency | 135725.9 ms |

#### Token Usage & Cost

| Metric | Value |
|--------|-------|
| Input Tokens | 79,132 |
| Output Tokens | 1,994,703 |
| Total Tokens | 2,073,835 |
| Total Cost | $30.3162 |

### Confusion Matrix


```
                 Predicted
              Yes        No
           ┌─────────┬─────────┐
Actual Yes │     43   │     78   │
           ├─────────┼─────────┤
Actual No  │     26   │    153   │
           └─────────┴─────────┘
```


- **True Positives (TP):** 43 - Correctly predicted "yes"
- **True Negatives (TN):** 153 - Correctly predicted "no"
- **False Positives (FP):** 26 - Incorrectly predicted "yes" (actual was "no")
- **False Negatives (FN):** 78 - Incorrectly predicted "no" (actual was "yes")

### Reliability Diagram

Shows calibration: ideally, avg_accuracy should equal avg_confidence in each bin.

| Bin | Avg Confidence | Avg Accuracy | Count | Gap |
|-----|----------------|--------------|-------|-----|
| 0.0-0.1 | 0.030 | 0.000 | 1 | +0.030 |
| 0.5-0.6 | 0.573 | 0.429 | 42 | +0.144 |
| 0.6-0.7 | 0.661 | 0.480 | 50 | +0.181 |
| 0.7-0.8 | 0.751 | 0.488 | 41 | +0.263 |
| 0.8-0.9 | 0.835 | 0.387 | 62 | +0.448 |
| 0.9-1.0 | 0.959 | 0.337 | 104 | +0.622 |


**Interpretation:**
- **Gap > 0:** Model is overconfident (confidence > accuracy)
- **Gap < 0:** Model is underconfident (confidence < accuracy)
- **Gap ≈ 0:** Well-calibrated

### Category Breakdown

| Category | Accuracy | Brier Score | Count |
|----------|----------|-------------|-------|
| Sports | 73.49% | 0.4209 | 83 |
| Politics | 60.00% | 0.3404 | 55 |
| Entertainment | 68.09% | 0.5435 | 47 |
| Companies | 60.00% | 0.3041 | 30 |
| Elections | 83.33% | 0.6944 | 24 |
| Mentions | 57.89% | 0.4298 | 19 |
| Crypto | 36.36% | 0.3916 | 11 |
| Climate and Weather | 66.67% | 0.4488 | 9 |
| Financials | 50.00% | 0.4176 | 8 |
| World | 66.67% | 0.3748 | 6 |
| Economics | 25.00% | 0.4100 | 4 |
| Social | 66.67% | 0.3562 | 3 |
| Science and Technology | 0.00% | 0.0625 | 1 |



---

## Qwen3-235B-Thinking

**Model Configuration:**
- **Provider:** ModelProvider.TOGETHER
- **Model ID:** together_ai/Qwen/Qwen3-235B-A22B-Thinking-2507
- **Knowledge Cutoff:** 2025-06-01
- **Temperature:** 0.7

### All Metrics

#### Classification Metrics

| Metric | Value |
|--------|-------|
| Accuracy | 0.6567 (65.67%) |
| Macro F1 | 0.6066 |
| Precision (Yes) | 0.6250 |
| Recall (Yes) | 0.3719 |
| F1 (Yes) | 0.4663 |
| Precision (No) | 0.6667 |
| Recall (No) | 0.8492 |
| F1 (No) | 0.7469 |

#### Calibration Metrics

| Metric | Value | Interpretation |
|--------|-------|----------------|
| Brier Score | 0.3459 | Lower is better (0 = perfect) |
| Brier Skill Score | -0.4372 | Higher is better (improvement over base rate) |
| ECE | 0.2966 | Lower is better (expected calibration error) |
| MCE | 0.4790 | Lower is better (max calibration error) |
| ACE | 0.3000 | Lower is better (adaptive calibration error) |
| Log Loss | 1.2726 | Lower is better |

#### Confidence Analysis

| Metric | Value |
|--------|-------|
| Average Confidence | 0.8166 |
| Avg Confidence When Correct | 0.8231472081218274 |
| Avg Confidence When Wrong | 0.8041747572815534 |
| Overconfidence Rate @70% | 0.3225806451612903 |
| Overconfidence Rate @80% | 0.32558139534883723 |
| Overconfidence Rate @90% | 0.3235294117647059 |

#### Performance Metrics

| Metric | Value |
|--------|-------|
| Parse Rate | 1.0000 (100.00%) |
| Avg Latency | 25397.5 ms |

#### Token Usage & Cost

| Metric | Value |
|--------|-------|
| Input Tokens | 83,267 |
| Output Tokens | 510,814 |
| Total Tokens | 594,081 |
| Total Cost | $1.1882 |

### Confusion Matrix


```
                 Predicted
              Yes        No
           ┌─────────┬─────────┐
Actual Yes │     45   │     76   │
           ├─────────┼─────────┤
Actual No  │     27   │    152   │
           └─────────┴─────────┘
```


- **True Positives (TP):** 45 - Correctly predicted "yes"
- **True Negatives (TN):** 152 - Correctly predicted "no"
- **False Positives (FP):** 27 - Incorrectly predicted "yes" (actual was "no")
- **False Negatives (FN):** 76 - Incorrectly predicted "no" (actual was "yes")

### Reliability Diagram

Shows calibration: ideally, avg_accuracy should equal avg_confidence in each bin.

| Bin | Avg Confidence | Avg Accuracy | Count | Gap |
|-----|----------------|--------------|-------|-----|
| 0.0-0.1 | 0.039 | 0.356 | 73 | -0.317 |
| 0.1-0.2 | 0.153 | 0.316 | 19 | -0.163 |
| 0.2-0.3 | 0.262 | 0.400 | 5 | -0.138 |
| 0.3-0.4 | 0.341 | 0.357 | 14 | -0.016 |
| 0.4-0.5 | 0.442 | 0.500 | 6 | -0.058 |
| 0.5-0.6 | 0.556 | 0.455 | 22 | +0.101 |
| 0.6-0.7 | 0.664 | 0.439 | 41 | +0.225 |
| 0.7-0.8 | 0.756 | 0.310 | 29 | +0.446 |
| 0.8-0.9 | 0.846 | 0.469 | 49 | +0.376 |
| 0.9-1.0 | 0.941 | 0.462 | 39 | +0.479 |


**Interpretation:**
- **Gap > 0:** Model is overconfident (confidence > accuracy)
- **Gap < 0:** Model is underconfident (confidence < accuracy)
- **Gap ≈ 0:** Well-calibrated

### Category Breakdown

| Category | Accuracy | Brier Score | Count |
|----------|----------|-------------|-------|
| Sports | 72.29% | 0.3490 | 83 |
| Politics | 58.18% | 0.3530 | 55 |
| Entertainment | 68.09% | 0.3540 | 47 |
| Companies | 63.33% | 0.3481 | 30 |
| Elections | 79.17% | 0.2752 | 24 |
| Mentions | 63.16% | 0.4352 | 19 |
| Crypto | 63.64% | 0.2471 | 11 |
| Climate and Weather | 55.56% | 0.2997 | 9 |
| Financials | 62.50% | 0.3272 | 8 |
| World | 50.00% | 0.3538 | 6 |
| Economics | 25.00% | 0.5620 | 4 |
| Social | 66.67% | 0.0963 | 3 |
| Science and Technology | 0.00% | 0.7225 | 1 |



---

## Kimi-K2

**Model Configuration:**
- **Provider:** ModelProvider.FIREWORKS
- **Model ID:** fireworks_ai/accounts/fireworks/models/kimi-k2-thinking
- **Knowledge Cutoff:** 2025-06-01
- **Temperature:** 0.7

### All Metrics

#### Classification Metrics

| Metric | Value |
|--------|-------|
| Accuracy | 0.6712 (67.12%) |
| Macro F1 | 0.6332 |
| Precision (Yes) | 0.6296 |
| Recall (Yes) | 0.4359 |
| F1 (Yes) | 0.5152 |
| Precision (No) | 0.6872 |
| Recall (No) | 0.8286 |
| F1 (No) | 0.7513 |

#### Calibration Metrics

| Metric | Value | Interpretation |
|--------|-------|----------------|
| Brier Score | 0.3473 | Lower is better (0 = perfect) |
| Brier Skill Score | -0.4461 | Higher is better (improvement over base rate) |
| ECE | 0.2981 | Lower is better (expected calibration error) |
| MCE | 0.5705 | Lower is better (max calibration error) |
| ACE | 0.2889 | Lower is better (adaptive calibration error) |
| Log Loss | 1.0835 | Lower is better |

#### Confidence Analysis

| Metric | Value |
|--------|-------|
| Average Confidence | 0.7935 |
| Avg Confidence When Correct | 0.8083673469387755 |
| Avg Confidence When Wrong | 0.7631249999999999 |
| Overconfidence Rate @70% | 0.25906735751295334 |
| Overconfidence Rate @80% | 0.2986111111111111 |
| Overconfidence Rate @90% | 0.3111111111111111 |

#### Performance Metrics

| Metric | Value |
|--------|-------|
| Parse Rate | 0.9733 (97.33%) |
| Avg Latency | 16206.0 ms |

#### Token Usage & Cost

| Metric | Value |
|--------|-------|
| Input Tokens | 79,112 |
| Output Tokens | 544,743 |
| Total Tokens | 623,855 |
| Total Cost | $0.9358 |

### Confusion Matrix


```
                 Predicted
              Yes        No
           ┌─────────┬─────────┐
Actual Yes │     51   │     66   │
           ├─────────┼─────────┤
Actual No  │     30   │    145   │
           └─────────┴─────────┘
```


- **True Positives (TP):** 51 - Correctly predicted "yes"
- **True Negatives (TN):** 145 - Correctly predicted "no"
- **False Positives (FP):** 30 - Incorrectly predicted "yes" (actual was "no")
- **False Negatives (FN):** 66 - Incorrectly predicted "no" (actual was "yes")

### Reliability Diagram

Shows calibration: ideally, avg_accuracy should equal avg_confidence in each bin.

| Bin | Avg Confidence | Avg Accuracy | Count | Gap |
|-----|----------------|--------------|-------|-----|
| 0.0-0.1 | 0.047 | 0.263 | 38 | -0.216 |
| 0.1-0.2 | 0.141 | 0.312 | 16 | -0.172 |
| 0.2-0.3 | 0.249 | 0.111 | 9 | +0.138 |
| 0.3-0.4 | 0.314 | 0.600 | 5 | -0.286 |
| 0.4-0.5 | 0.465 | 0.000 | 2 | +0.465 |
| 0.5-0.6 | 0.570 | 0.477 | 44 | +0.093 |
| 0.6-0.7 | 0.668 | 0.458 | 48 | +0.210 |
| 0.7-0.8 | 0.750 | 0.484 | 31 | +0.266 |
| 0.8-0.9 | 0.849 | 0.447 | 38 | +0.402 |
| 0.9-1.0 | 0.948 | 0.377 | 61 | +0.570 |


**Interpretation:**
- **Gap > 0:** Model is overconfident (confidence > accuracy)
- **Gap < 0:** Model is underconfident (confidence < accuracy)
- **Gap ≈ 0:** Well-calibrated

### Category Breakdown

| Category | Accuracy | Brier Score | Count |
|----------|----------|-------------|-------|
| Sports | 75.00% | 0.3103 | 80 |
| Politics | 55.56% | 0.3198 | 54 |
| Entertainment | 66.67% | 0.4233 | 45 |
| Companies | 63.33% | 0.3049 | 30 |
| Elections | 95.65% | 0.3698 | 23 |
| Mentions | 63.16% | 0.4462 | 19 |
| Crypto | 60.00% | 0.2837 | 10 |
| Climate and Weather | 66.67% | 0.3135 | 9 |
| Financials | 62.50% | 0.4131 | 8 |
| World | 50.00% | 0.3468 | 6 |
| Economics | 50.00% | 0.3295 | 4 |
| Social | 33.33% | 0.4238 | 3 |
| Science and Technology | 0.00% | 0.4900 | 1 |




---

## Files Generated

| File | Description |
|------|-------------|
| `summary_*.json` | Aggregated results for all models |
| `<model>_*.json` | Individual model results with all predictions |
| `metadata_*.json` | Benchmark configuration and dataset statistics |
| `dataset_analysis_*.json` | Comprehensive dataset analysis |
| `paper_methods_prompt_*.txt` | Prompt for generating Methods section |
| `paper_results_prompt_*.txt` | Prompt for generating Results section |
| `paper_dataset_prompt_*.txt` | Prompt for generating Dataset section |
| `report_*.md` | This report |

---

## Citation

```bibtex
@misc{kalshibench2024,
  title={KalshiBench: Evaluating LLM Forecasting Calibration via Prediction Markets},
  author={2084 Collective},
  year={2024},
  note={Evaluation of 5 models on 300 prediction market questions}
}
```

---

*Report generated by KalshiBench v1.0*
